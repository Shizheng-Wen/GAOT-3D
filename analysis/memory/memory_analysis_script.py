#!/usr/bin/env python3
"""
æ˜¾å­˜åˆ†æè„šæœ¬ï¼šæ¢ç©¶MAGNO/GAOT-3Dä¸­edgesæ•°é‡å¯¹æ˜¾å­˜å ç”¨å³°å€¼çš„å½±å“

ä¸»è¦åˆ†æï¼š
1. ä¸åŒedgesæ•°é‡ä¸‹çš„æ˜¾å­˜ä½¿ç”¨
2. è®­ç»ƒvsæ¨ç†æ¨¡å¼çš„æ˜¾å­˜å·®å¼‚  
3. encoder vs decoderçš„æ˜¾å­˜å¼€é”€
4. æä¾›è¯¦ç»†çš„åˆ†ææŠ¥å‘Šå’Œå¯è§†åŒ–

ä½¿ç”¨æ–¹æ³•ï¼š
python memory_analysis_script.py
"""

import os
import sys
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from typing import List, Tuple, Dict, Optional
import gc
import psutil
import time
from dataclasses import dataclass
from torch_geometric.data import Data, Batch
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ srcè·¯å¾„ä»¥å¯¼å…¥é¡¹ç›®æ¨¡å—
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from src.model.layers.magno import MAGNOEncoder, MAGNODecoder, MAGNOConfig
from src.model.gaot_3d import GAOT3D
from src.model.layers.integral_transform import IntegralTransform


@dataclass
class MemoryTestConfig:
    """å†…å­˜æµ‹è¯•é…ç½®"""
    device: str = "cuda:0"
    batch_size: int = 1
    num_physical_nodes: int = 5000      # ç‰©ç†ç½‘æ ¼èŠ‚ç‚¹æ•°
    num_latent_tokens: int = 4096       # æ½œåœ¨tokenæ•° (16x16x16)
    coord_dim: int = 3                  # 3Dåæ ‡
    input_channels: int = 4             # è¾“å…¥ç‰¹å¾ç»´åº¦
    output_channels: int = 1            # è¾“å‡ºç‰¹å¾ç»´åº¦
    lifting_channels: int = 64          # MAGNOé€šé“æ•°
    
    # edgesæ•°é‡æµ‹è¯•èŒƒå›´
    min_edges: int = 10000             # æœ€å°edgesæ•°
    max_edges: int = 1000000           # æœ€å¤§edgesæ•°  
    num_test_points: int = 15          # æµ‹è¯•ç‚¹æ•°é‡
    
    # æµ‹è¯•é€‰é¡¹
    test_encoder: bool = True
    test_decoder: bool = True
    test_full_model: bool = True
    test_training_mode: bool = True
    test_inference_mode: bool = True


class MemoryProfiler:
    """GPUæ˜¾å­˜ç›‘æ§å™¨"""
    
    def __init__(self, device: str):
        self.device = device
        if torch.cuda.is_available() and device.startswith('cuda'):
            self.device_id = int(device.split(':')[1]) if ':' in device else 0
        else:
            raise ValueError("CUDA not available or invalid device")
    
    def get_memory_info(self) -> Dict[str, float]:
        """è·å–å½“å‰æ˜¾å­˜ä¿¡æ¯ (MB)"""
        torch.cuda.synchronize(self.device_id)
        allocated = torch.cuda.memory_allocated(self.device_id) / 1024**2
        reserved = torch.cuda.memory_reserved(self.device_id) / 1024**2
        max_allocated = torch.cuda.max_memory_allocated(self.device_id) / 1024**2
        max_reserved = torch.cuda.max_memory_reserved(self.device_id) / 1024**2
        
        return {
            'allocated': allocated,
            'reserved': reserved, 
            'max_allocated': max_allocated,
            'max_reserved': max_reserved
        }
    
    def reset_peak_memory(self):
        """é‡ç½®å³°å€¼æ˜¾å­˜ç»Ÿè®¡"""
        torch.cuda.reset_peak_memory_stats(self.device_id)
    
    def clear_cache(self):
        """æ¸…ç†æ˜¾å­˜ç¼“å­˜"""
        torch.cuda.empty_cache()
        gc.collect()


class SyntheticDataGenerator:
    """åˆæˆæµ‹è¯•æ•°æ®ç”Ÿæˆå™¨"""
    
    @staticmethod
    def generate_batch(config: MemoryTestConfig, num_edges: int) -> Batch:
        """ç”ŸæˆæŒ‡å®šedgesæ•°é‡çš„PyG Batch"""
        
        # ç”Ÿæˆç‰©ç†èŠ‚ç‚¹ä½ç½®
        phys_pos = torch.randn(config.num_physical_nodes, config.coord_dim, dtype=torch.float32)
        phys_feat = torch.randn(config.num_physical_nodes, config.input_channels, dtype=torch.float32)
        
        # ç”Ÿæˆéšæœºedges (ç¡®ä¿ä¸è¶…è¿‡å¯èƒ½çš„æœ€å¤§edgesæ•°)
        max_possible_edges = config.num_physical_nodes * config.num_latent_tokens
        actual_num_edges = min(num_edges, max_possible_edges)
        
        # éšæœºç”Ÿæˆedge_index
        edge_index = torch.randint(0, config.num_physical_nodes, (2, actual_num_edges), dtype=torch.long)
        
        # åˆ›å»ºbatch
        data = Data(
            pos=phys_pos,
            x=phys_feat,
            edge_index=edge_index,
            batch=torch.zeros(config.num_physical_nodes, dtype=torch.long)
        )
        
        batch = Batch.from_data_list([data])
        return batch, actual_num_edges
    
    @staticmethod 
    def generate_latent_tokens(config: MemoryTestConfig, device: str) -> Tuple[torch.Tensor, torch.Tensor]:
        """ç”Ÿæˆæ½œåœ¨tokens"""
        # åˆ›å»ºè§„å¾‹åŒ–çš„3Dç½‘æ ¼
        D = H = W = int(config.num_latent_tokens**(1/3))  # å‡è®¾ç«‹æ–¹ä½“ç½‘æ ¼
        x = torch.linspace(-1, 1, D)
        y = torch.linspace(-1, 1, H) 
        z = torch.linspace(-1, 1, W)
        
        mesh_x, mesh_y, mesh_z = torch.meshgrid(x, y, z, indexing='ij')
        latent_pos = torch.stack([mesh_x.flatten(), mesh_y.flatten(), mesh_z.flatten()], dim=1)
        latent_batch = torch.zeros(latent_pos.shape[0], dtype=torch.long)
        
        return latent_pos.to(device), latent_batch.to(device)


class ComponentTester:
    """ç»„ä»¶çº§æ˜¾å­˜æµ‹è¯•å™¨"""
    
    def __init__(self, config: MemoryTestConfig):
        self.config = config
        self.profiler = MemoryProfiler(config.device)
        self.device = config.device
        
        # åˆ›å»ºMAGNOé…ç½®
        self.magno_config = MAGNOConfig(
            gno_coord_dim=config.coord_dim,
            lifting_channels=config.lifting_channels,
            in_gno_channel_mlp_hidden_layers=[64, 64, 64],
            out_gno_channel_mlp_hidden_layers=[64, 64],
            precompute_edges=False  # ä¸ä½¿ç”¨é¢„è®¡ç®—edgesä»¥ä¾¿è‡ªç”±æ§åˆ¶
        )
    
    def test_encoder_memory(self, num_edges: int, training_mode: bool = True) -> Dict[str, float]:
        """æµ‹è¯•encoderæ˜¾å­˜ä½¿ç”¨"""
        self.profiler.clear_cache()
        self.profiler.reset_peak_memory()
        
        try:
            # åˆ›å»ºencoder
            encoder = MAGNOEncoder(
                in_channels=self.config.input_channels,
                out_channels=self.config.lifting_channels,
                gno_config=self.magno_config
            ).to(self.device)
            
            if training_mode:
                encoder.train()
            else:
                encoder.eval()
            
            # ç”Ÿæˆæµ‹è¯•æ•°æ®
            batch, actual_edges = SyntheticDataGenerator.generate_batch(self.config, num_edges)
            batch = batch.to(self.device)
            
            latent_pos, latent_batch = SyntheticDataGenerator.generate_latent_tokens(self.config, self.device)
            
            initial_memory = self.profiler.get_memory_info()
            
            # å‰å‘ä¼ æ’­
            with torch.set_grad_enabled(training_mode):
                if training_mode:
                    # è®­ç»ƒæ¨¡å¼ï¼šè®¡ç®—æ¢¯åº¦
                    encoded = encoder(batch, latent_pos, latent_batch)
                    loss = encoded.sum()  # ç®€å•çš„æŸå¤±å‡½æ•°
                    loss.backward()
                else:
                    # æ¨ç†æ¨¡å¼
                    with torch.no_grad():
                        encoded = encoder(batch, latent_pos, latent_batch)
            
            peak_memory = self.profiler.get_memory_info()
            
            # æ¸…ç†
            del encoder, batch, encoded
            if training_mode and 'loss' in locals():
                del loss
            
            return {
                'actual_edges': actual_edges,
                'peak_allocated': peak_memory['max_allocated'],
                'peak_reserved': peak_memory['max_reserved'],
                'memory_increase': peak_memory['max_allocated'] - initial_memory['allocated']
            }
            
        except RuntimeError as e:
            if "out of memory" in str(e):
                return {
                    'actual_edges': num_edges,
                    'peak_allocated': float('inf'),
                    'peak_reserved': float('inf'), 
                    'memory_increase': float('inf'),
                    'oom_error': True
                }
            else:
                raise e
    
    def test_decoder_memory(self, num_edges: int, training_mode: bool = True) -> Dict[str, float]:
        """æµ‹è¯•decoderæ˜¾å­˜ä½¿ç”¨"""
        self.profiler.clear_cache()
        self.profiler.reset_peak_memory()
        
        try:
            # åˆ›å»ºdecoder
            decoder = GNODecoder(
                in_channels=self.config.lifting_channels,
                out_channels=self.config.output_channels,
                gno_config=self.magno_config
            ).to(self.device)
            
            if training_mode:
                decoder.train()
            else:
                decoder.eval()
            
            # ç”Ÿæˆæµ‹è¯•æ•°æ®
            batch, actual_edges = SyntheticDataGenerator.generate_batch(self.config, num_edges)
            batch = batch.to(self.device)
            
            latent_pos, latent_batch = SyntheticDataGenerator.generate_latent_tokens(self.config, self.device)
            
            # åˆ›å»ºæ½œåœ¨ç‰¹å¾æ•°æ®
            latent_features = torch.randn(
                self.config.num_latent_tokens, 
                self.config.lifting_channels,
                device=self.device
            )
            
            initial_memory = self.profiler.get_memory_info()
            
            # å‰å‘ä¼ æ’­
            with torch.set_grad_enabled(training_mode):
                if training_mode:
                    decoded = decoder(
                        rndata_flat=latent_features,
                        phys_pos_query=batch.pos,
                        batch_idx_phys_query=batch.batch,
                        latent_tokens_pos=latent_pos,
                        latent_tokens_batch_idx=latent_batch,
                        batch=None  # ä¸ä½¿ç”¨é¢„è®¡ç®—edges
                    )
                    loss = decoded.sum()
                    loss.backward()
                else:
                    with torch.no_grad():
                        decoded = decoder(
                            rndata_flat=latent_features,
                            phys_pos_query=batch.pos,
                            batch_idx_phys_query=batch.batch,
                            latent_tokens_pos=latent_pos,
                            latent_tokens_batch_idx=latent_batch,
                            batch=None
                        )
            
            peak_memory = self.profiler.get_memory_info()
            
            # æ¸…ç†
            del decoder, batch, decoded, latent_features
            if training_mode and 'loss' in locals():
                del loss
            
            return {
                'actual_edges': actual_edges,
                'peak_allocated': peak_memory['max_allocated'],
                'peak_reserved': peak_memory['max_reserved'],
                'memory_increase': peak_memory['max_allocated'] - initial_memory['allocated']
            }
            
        except RuntimeError as e:
            if "out of memory" in str(e):
                return {
                    'actual_edges': num_edges,
                    'peak_allocated': float('inf'),
                    'peak_reserved': float('inf'),
                    'memory_increase': float('inf'),
                    'oom_error': True
                }
            else:
                raise e
    
    def test_full_model_memory(self, num_edges: int, training_mode: bool = True) -> Dict[str, float]:
        """æµ‹è¯•å®Œæ•´GAOT-3Dæ¨¡å‹æ˜¾å­˜ä½¿ç”¨"""
        self.profiler.clear_cache()
        self.profiler.reset_peak_memory()
        
        try:
            # åˆ›å»ºå®Œæ•´æ¨¡å‹ (ç®€åŒ–çš„transformeré…ç½®)
            from src.model.layers.attn import TransformerConfig
            
            transformer_config = TransformerConfig(
                patch_size=4,  # å°patché¿å…æ˜¾å­˜è¿‡å¤§
                hidden_size=256,
                num_layers=2,
                positional_embedding='absolute'
            )
            
            model = GAOT3D(
                input_size=self.config.input_channels,
                output_size=self.config.output_channels,
                magno_config=self.magno_config,
                attn_config=transformer_config,
                latent_tokens=(16, 16, 16)  # 4096 tokens
            ).to(self.device)
            
            if training_mode:
                model.train()
            else:
                model.eval()
            
            # ç”Ÿæˆæµ‹è¯•æ•°æ®
            batch, actual_edges = SyntheticDataGenerator.generate_batch(self.config, num_edges)
            batch = batch.to(self.device)
            
            initial_memory = self.profiler.get_memory_info()
            
            # å‰å‘ä¼ æ’­
            with torch.set_grad_enabled(training_mode):
                if training_mode:
                    output = model(batch)
                    loss = output.sum()
                    loss.backward()
                else:
                    with torch.no_grad():
                        output = model(batch)
            
            peak_memory = self.profiler.get_memory_info()
            
            # æ¸…ç†
            del model, batch, output
            if training_mode and 'loss' in locals():
                del loss
            
            return {
                'actual_edges': actual_edges,
                'peak_allocated': peak_memory['max_allocated'],
                'peak_reserved': peak_memory['max_reserved'],
                'memory_increase': peak_memory['max_allocated'] - initial_memory['allocated']
            }
            
        except RuntimeError as e:
            if "out of memory" in str(e):
                return {
                    'actual_edges': num_edges,
                    'peak_allocated': float('inf'),
                    'peak_reserved': float('inf'),
                    'memory_increase': float('inf'),
                    'oom_error': True
                }
            else:
                raise e


class MemoryAnalyzer:
    """æ˜¾å­˜åˆ†æä¸»ç±»"""
    
    def __init__(self, config: MemoryTestConfig):
        self.config = config
        self.tester = ComponentTester(config)
        self.results = {}
    
    def run_comprehensive_analysis(self) -> Dict[str, pd.DataFrame]:
        """è¿è¡Œå…¨é¢çš„æ˜¾å­˜åˆ†æ"""
        print("ğŸš€ å¼€å§‹GAOT-3Dæ˜¾å­˜åˆ†æ...")
        print(f"ğŸ“Š æµ‹è¯•é…ç½®: {self.config.num_test_points}ä¸ªedgesè§„æ¨¡ç‚¹")
        print(f"ğŸ¯ EdgesèŒƒå›´: {self.config.min_edges:,} - {self.config.max_edges:,}")
        print(f"ğŸ”§ è®¾å¤‡: {self.config.device}")
        print(f"ğŸ§® ç‰©ç†èŠ‚ç‚¹æ•°: {self.config.num_physical_nodes:,}")
        print(f"ğŸ”® æ½œåœ¨Tokensæ•°: {self.config.num_latent_tokens:,}")
        print("="*80)
        
        # ç”Ÿæˆæµ‹è¯•çš„edgesæ•°é‡èŒƒå›´ (å¯¹æ•°åˆ†å¸ƒ)
        edges_range = np.logspace(
            np.log10(self.config.min_edges),
            np.log10(self.config.max_edges),
            self.config.num_test_points
        ).astype(int)
        
        test_scenarios = []
        
        # å®šä¹‰æ‰€æœ‰æµ‹è¯•åœºæ™¯
        if self.config.test_encoder:
            if self.config.test_training_mode:
                test_scenarios.append(('encoder', 'training'))
            if self.config.test_inference_mode:
                test_scenarios.append(('encoder', 'inference'))
        
        if self.config.test_decoder:
            if self.config.test_training_mode:
                test_scenarios.append(('decoder', 'training'))
            if self.config.test_inference_mode:
                test_scenarios.append(('decoder', 'inference'))
        
        if self.config.test_full_model:
            if self.config.test_training_mode:
                test_scenarios.append(('full_model', 'training'))
            if self.config.test_inference_mode:
                test_scenarios.append(('full_model', 'inference'))
        
        # æ‰§è¡Œæ‰€æœ‰æµ‹è¯•
        for component, mode in test_scenarios:
            print(f"\nğŸ§ª æµ‹è¯• {component.upper()} - {mode.upper()} æ¨¡å¼")
            
            results = []
            for i, num_edges in enumerate(edges_range):
                print(f"  ğŸ“ [{i+1:2d}/{len(edges_range)}] Edges: {num_edges:8,}", end=" -> ")
                
                # æ ¹æ®ç»„ä»¶ç±»å‹é€‰æ‹©æµ‹è¯•å‡½æ•°
                if component == 'encoder':
                    result = self.tester.test_encoder_memory(num_edges, mode=='training')
                elif component == 'decoder':
                    result = self.tester.test_decoder_memory(num_edges, mode=='training')
                elif component == 'full_model':
                    result = self.tester.test_full_model_memory(num_edges, mode=='training')
                
                # æ·»åŠ æµ‹è¯•ä¿¡æ¯
                result.update({
                    'component': component,
                    'mode': mode,
                    'target_edges': num_edges
                })
                
                results.append(result)
                
                # è¾“å‡ºç»“æœ
                if 'oom_error' in result:
                    print("ğŸ’¥ OOM!")
                    break
                else:
                    print(f"ğŸ’¾ {result['peak_allocated']:.1f}MB")
                
                time.sleep(0.1)  # çŸ­æš‚å»¶è¿Ÿè®©æ˜¾å­˜ç¨³å®š
            
            # ä¿å­˜ç»“æœ
            key = f"{component}_{mode}"
            self.results[key] = pd.DataFrame(results)
        
        print("\nâœ… åˆ†æå®Œæˆ!")
        return self.results
    
    def generate_report(self) -> str:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        report = []
        report.append("ğŸ“Š GAOT-3D æ˜¾å­˜åˆ†ææŠ¥å‘Š")
        report.append("="*60)
        report.append("")
        
        for key, df in self.results.items():
            component, mode = key.split('_', 1)
            report.append(f"ğŸ” {component.upper()} - {mode.upper()}æ¨¡å¼:")
            
            if df.empty:
                report.append("  âŒ æ— æ•°æ®")
                continue
            
            # è¿‡æ»¤æ‰OOMçš„ç»“æœ - ä¿®å¤pandas DataFrameçš„ä½¿ç”¨
            if 'oom_error' in df.columns:
                valid_df = df[~df['oom_error'].fillna(False)]
            else:
                valid_df = df  # å¦‚æœæ²¡æœ‰oom_erroråˆ—ï¼Œè¯´æ˜æ²¡æœ‰OOM
            
            if valid_df.empty:
                report.append("  ğŸ’¥ æ‰€æœ‰æµ‹è¯•éƒ½å‘ç”ŸOOM")
                continue
            
            max_edges = valid_df['actual_edges'].max()
            max_memory = valid_df['peak_allocated'].max()
            min_memory = valid_df['peak_allocated'].min()
            
            report.append(f"  ğŸ“ æœ€å¤§æˆåŠŸedgesæ•°: {max_edges:,}")
            report.append(f"  ğŸ’¾ å³°å€¼æ˜¾å­˜: {max_memory:.1f}MB")
            report.append(f"  ğŸ’š æœ€å°æ˜¾å­˜: {min_memory:.1f}MB")
            report.append(f"  ğŸ“ˆ æ˜¾å­˜å¢é•¿: {max_memory/min_memory:.2f}x")
            report.append("")
        
        return "\n".join(report)
    
    def plot_results(self, save_path: str = "memory_analysis_results.png"):
        """ç»˜åˆ¶åˆ†æç»“æœå›¾è¡¨"""
        if not self.results:
            print("âŒ æ— ç»“æœæ•°æ®å¯ç»˜åˆ¶")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        axes = axes.flatten()
        
        plot_idx = 0
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7', '#DDA0DD']
        
        for key, df in self.results.items():
            if df.empty or plot_idx >= 4:
                continue
            
            component, mode = key.split('_', 1)
            
            # è¿‡æ»¤æœ‰æ•ˆæ•°æ® - ä¿®å¤pandas DataFrameçš„ä½¿ç”¨
            if 'oom_error' in df.columns:
                valid_df = df[~df['oom_error'].fillna(False)]
            else:
                valid_df = df  # å¦‚æœæ²¡æœ‰oom_erroråˆ—ï¼Œè¯´æ˜æ²¡æœ‰OOM
            
            if valid_df.empty:
                continue
            
            ax = axes[plot_idx]
            
            # ç»˜åˆ¶å³°å€¼æ˜¾å­˜ vs edgesæ•°é‡
            ax.loglog(valid_df['actual_edges'], valid_df['peak_allocated'], 
                     'o-', color=colors[plot_idx], linewidth=2, markersize=6,
                     label=f"{component.title()} ({mode})")
            
            ax.set_xlabel('è¾¹æ•°é‡ (Edges)', fontsize=12, fontweight='bold')
            ax.set_ylabel('å³°å€¼æ˜¾å­˜ (MB)', fontsize=12, fontweight='bold')
            ax.set_title(f'{component.upper()} - {mode.upper()}', fontsize=14, fontweight='bold')
            ax.grid(True, alpha=0.3)
            ax.legend()
            
            # æ·»åŠ æ‹Ÿåˆçº¿
            if len(valid_df) > 3:
                try:
                    log_edges = np.log10(valid_df['actual_edges'])
                    log_memory = np.log10(valid_df['peak_allocated'])
                    
                    # çº¿æ€§æ‹Ÿåˆ (åœ¨å¯¹æ•°ç©ºé—´)
                    coeff = np.polyfit(log_edges, log_memory, 1)
                    
                    # ç»˜åˆ¶æ‹Ÿåˆçº¿
                    x_fit = np.logspace(np.log10(valid_df['actual_edges'].min()), 
                                      np.log10(valid_df['actual_edges'].max()), 100)
                    y_fit = 10**(coeff[0] * np.log10(x_fit) + coeff[1])
                    
                    ax.plot(x_fit, y_fit, '--', color=colors[plot_idx], alpha=0.7,
                           label=f'Slope: {coeff[0]:.2f}')
                    ax.legend()
                    
                except:
                    pass
            
            plot_idx += 1
        
        # ç§»é™¤æœªä½¿ç”¨çš„å­å›¾
        for i in range(plot_idx, 4):
            fig.delaxes(axes[i])
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“ˆ ç»“æœå›¾è¡¨å·²ä¿å­˜è‡³: {save_path}")
        plt.show()
    
    def save_raw_data(self, save_path: str = "memory_analysis_data.csv"):
        """ä¿å­˜åŸå§‹åˆ†ææ•°æ®"""
        if not self.results:
            print("âŒ æ— æ•°æ®å¯ä¿å­˜")
            return
        
        # åˆå¹¶æ‰€æœ‰ç»“æœ
        all_data = []
        for key, df in self.results.items():
            if not df.empty:
                all_data.append(df)
        
        if all_data:
            combined_df = pd.concat(all_data, ignore_index=True)
            combined_df.to_csv(save_path, index=False)
            print(f"ğŸ’¾ åŸå§‹æ•°æ®å·²ä¿å­˜è‡³: {save_path}")
        else:
            print("âŒ æ— æœ‰æ•ˆæ•°æ®å¯ä¿å­˜")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§  GAOT-3D æ˜¾å­˜åˆ†æå·¥å…·")
    print("="*50)
    
    # æ£€æŸ¥CUDAå¯ç”¨æ€§
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼è¯·åœ¨GPUç¯å¢ƒä¸­è¿è¡Œæ­¤è„šæœ¬")
        return
    
    # æ˜¾ç¤ºGPUä¿¡æ¯
    gpu_name = torch.cuda.get_device_name(0)
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    print(f"ğŸ® GPU: {gpu_name}")
    print(f"ğŸ’¾ æ˜¾å­˜: {gpu_memory:.1f}GB")
    print()
    
    save_folder = "results/memory_analysis"
    if os.path.exists(save_folder) is False:
        os.makedirs(save_folder, exist_ok=True)
    save_path = os.path.join(save_folder, "memory_analysis_results.png")

    # åˆ›å»ºæµ‹è¯•é…ç½®
    config = MemoryTestConfig(
        device="cuda:0",
        batch_size=1,
        num_physical_nodes=500000,
        num_latent_tokens=4096,
        min_edges=500000,
        max_edges=5000000,  # æ ¹æ®ä½ çš„GPUè°ƒæ•´
        num_test_points=12,
        test_encoder=True,
        test_decoder=True,
        test_full_model=True,  # å®Œæ•´æ¨¡å‹æµ‹è¯•å¯èƒ½å¾ˆæ…¢
        test_training_mode=True,
        test_inference_mode=True
    )
    
    # åˆ›å»ºåˆ†æå™¨å¹¶è¿è¡Œ
    analyzer = MemoryAnalyzer(config)
    
    try:
        results = analyzer.run_comprehensive_analysis()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = analyzer.generate_report()
        print("\n" + report)
        
        # ç»˜åˆ¶ç»“æœ
        analyzer.plot_results(save_path=save_path)
        
        # ä¿å­˜æ•°æ®
        analyzer.save_raw_data(save_path=os.path.join(save_folder, "memory_analysis_data.csv"))
        
        print("\nğŸ‰ åˆ†æå®Œæˆï¼")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­åˆ†æ")
    except Exception as e:
        print(f"\nâŒ åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 